import { Accordion, Code, Container, List, ListItem, Stack, Text, Title } from "@mantine/core";

const pageContent = (
  <>
    <Title>Computer System Overview</Title>

    <Title order={2}>Basic Elements</Title>
    <Text>
      At a high level, a computer&apos;s components can be broken down into a processor, main memory, I/O modules, and a
      system bus. When there is only one processor, it is generally called a CPU. Main memory is used to store both
      instructions and data; it is also known as real or primary memory. I/O modules move data between the computer and
      its external environment such as secondary memory, terminals, or other communication equipment. Finally, the
      system bus allows the different components to communicate.
    </Text>
    <Title order={5}>Processor</Title>
    <Text>
      Ideally, there would only be one source and destination of memory, that being e.g. an ultra-fast zero-latency
      secondary storage device that has no overhead when reading or writing; this way, there would be no need for memory
      compromises such as caching and registers. Unfortunately, this is not the case.
    </Text>
    <Text>
      Because the processor will constantly be reading from main memory, the processor has dedicated internal registers
      to communicate with main memory. These two registers are the Memory Address Register (MAR) and Memory Buffer
      Register (MBR). MAR specifies the memory address for the next read or write, and the MBR contains the data to be
      written into memory or the data that has just been read from memory.
    </Text>
    <Text>
      Similarly to how the processor has dedicated memory registers due to the commonplace of main memory interactions,
      I/O operations will also be commonplace, so the processor generally has two I/O specific registers. These two
      registers are the I/O Address Register (I/OAR) and the I/O Buffer Register (I/OBR). I/OAR specifies a certain I/O
      device. I/OBR is used for the exchange of data between the processor and I/O device, similar to the MBR.
    </Text>

    <Title order={2}>Evolution of the Microprocessor</Title>
    <Text>
      Originally, microprocessors were a single processor on a single chip. These were slower than multichip processors.
      Now, microprocessors have evolved to become multiprocessors, where each chip contains multiple processors. In the
      context of multiprocessors, a chip is known as a socket and a processor is known as a core. So, microprocessors
      evolved from one processor on one chip to multiple processors (now known as a core) on multiple chips (now known
      as sockets). So multichip?
    </Text>
    <Text>
      GPUs are fast at performing Single Instruction Multiple Data (SIMD) operations on arrays of data. CPUs are able to
      operate on arrays of data for SIMD operations through vector units. Digital Signal Processors (DSPs) are also
      available for handling streaming signals like audio / video.
    </Text>
    <Text>
      Systems on a Chip (SoC) are starting to take over microprocessors, where other components of the system like GPUs,
      DSPs, I/O devices, and main memory are being put on the same chip, not just CPUs and caches.
    </Text>

    <Title order={2}>Instruction Execution</Title>
    <Text>
      In order to perform instruction execution, the CPU makes use of several registers. The Program Counter (PC) holds
      the address of the next instruction to fetch for execution. The Instruction Register (IR) holds the instruction
      that is currently being executed (that was previously loaded in from the memory location that the PC was pointing
      to). At a high level, at the beginning of the instruction cycle, the instruction pointed to by the PC is fetched,
      placed into the IR, and the PC is incremented. The processesor interprets the instruction to execute and executes
      it. The cycle then continues.
    </Text>
    <Text>
      At a high level, any given instruction executed by the processor falls into one of the following four categories.
    </Text>
    <List>
      <ListItem>Processor &lt;-&gt; Memory</ListItem>
      <ListItem>Processor &lt;-&gt; I/O</ListItem>
      <ListItem>Data processing</ListItem>
      <ListItem>Control</ListItem>
    </List>
    <Text>
      Control instructions can cause the sequence of execution to change; i.e. a branching operation by modifying the
      PC.
    </Text>

    <Title order={2}>Interrupts</Title>
    <Text>
      Interrupts improve processor utilisation. Interrupts can be generated by programs, a timer within the processor,
      I/O, or hardware failures, as well as others. An interrupt is sent to the processor which sets its interrupt flag.
      Upon an interrupt, the processor will save the state of the current program and run an interrupt-handler routine.
      The interrupt-handler routine is typically part of the OS but is initiated by the processor? Interrupts are
      checked for after the execution of the current fetch-execute cycle.
    </Text>
    <Text>
      Upon receiving an interrupt request, the processor will send an acknowledgement to the device that caused it so
      that the device removes the interrupt signal. The processor will then store the current PC, Program Status Word
      (PSW), and processor registers onto a data structure known as the control stack; the processor registers are
      actually saved by the interrupt-handler? Then, the processor will need to load the entrypoint of the
      interrupt-handler program into the PC. The address may be available in the interrupt signal or it may need to be
      fetched from the device that issued the interrupt.
    </Text>
    <Title order={4}>Multiple Interrupts</Title>
    <Text>
      The idea of saving the state of the currently executing program to a control stack scales well when wanting to
      allow multiple interrupts. Allowing multiple interrupts is not so important from a processor utilisation point of
      view, but is more important when it comes to certain interrupts being more necessary to process than others. For
      example, time-critical interrupts where incoming data has a relatively small buffer space so must be cleared
      quickly but there is already an interrupt handler running for a less important module. In this case, it is ideal
      to be able to assign priorities to interrupts and interrupt existing Interrupt Service Routines (ISRs).
    </Text>

    <Title order={2}>The Memory Hierarchy</Title>
    <Text>
      Memory is typically a triplet of cost, capacity and speed, where typically only the ideal of two can be achieved.
      At the top of the hierarchy are registers, which are expensive, small (in terms of capacity), and fast. They are
      also accessed very frequently by the processor. At the bottom of the hierarchy are offline storage devices like
      magnetic tape. Magnetic tape is cheap, large (in terms of capacity), and slow. It is not accessed frequently by
      the processor at all.
    </Text>
    <Text>
      Having a memory hierarchy speeds up computation by taking advantage of the fact that processors typically
      frequently access data in clusters, known as locality of reference. Locality of reference can be taken advantage
      of by moving data the processor is working with from secondary storage (slower, cheaper, but larger) to a memory
      type that is much faster but slower. Processors will be able to access this memory much more frequently with less
      overhead.
    </Text>
    <Text>
      It is possible to calculate the average access time for memory when using e.g. a two-level hierarchy consisting of
      e.g. processor cache and main memory. Assume it takes 0.1us to read from cache memory and 1us to access from main
      memory. Had all memory accesses (100%) already been in cache, the average access time would be calculated via
    </Text>
    <Code>1 * 0.1us = 0.1us</Code>
    <Text>
      However, had 90% of memory accesses already been in processor cache and the remaining 10% in main memory, the main
      memory data would be forwarded into processor cache first. Therefore, the average access time would be calculated
      via
    </Text>
    <Code>(0.9 * 0.1us) + (0.1 * 1us) = 0.09us + 0.1us = 0.19us</Code>
    <Text>
      So, nearly twice as slow just for a 10% decrease in the amount of data that needed to be fetched that was already
      in processor cache.
    </Text>

    <Title order={2}>Cache Memory</Title>
    <Text>
      Cache memory is invisible to the OS and works at the hardware level. Due to locality of reference, it would be
      great if a block of memory in the same region / cluster could also be loaded into cache rather than just a
      particular machine word being accessed.
    </Text>
    <Text>
      Memory can be grouped into blocks of words, i.e. a block can contain <Code>K</Code> machine words. Assuming there
      are 2^n addressible locations of memory, where n is the number of bits in an address and each location in memory
      stores a machine word, then there are <Code>2^n / K</Code>
    </Text>

    <Title order={2}>Direct Memory Access</Title>
    <Text>There are three techniques for I/O operations</Text>
    <List>
      <ListItem>
        <Text fw={700}>Programmed I/O</Text>
        <Text>
          When the processor runs an I/O instruction it will issue a command to the respective I/O module. The I/O
          module will then a certain bit in the I/O register. Nothing else is done to indicate I/O completion. The
          processor can only poll the register to check completion, which is inefficient.
        </Text>
      </ListItem>
      <ListItem>Interrupt-driven I/O</ListItem>
      <ListItem>Direct Memory Access (DMA)</ListItem>
    </List>

    <Title order={2}>Multiprocessor and Multicore Organisation (DONE)</Title>
    <Text>
      There are a few ways to provide parallelisation by replicating processors: symmetric multiprocessors (SMPs),
      multicore computers, and clusters. This section deals with SMPs and multicore computers.
    </Text>
    <Title order={3}>Symmetric multiprocessors</Title>
    <Text>SMPs have the following properties</Text>
    <List>
      <ListItem>Two or more similar processors (in terms of specs and functions they can execute)</ListItem>
      <ListItem>Processors share same memory and I/O (whether through the same or a different channel)</ListItem>
      <ListItem>Controlled by an integrated OS, providing interaction between processors and their programs</ListItem>
    </List>
    <Text>They have the following advantages</Text>
    <List>
      <ListItem>Performance: when programs can be written to run on multiple processors</ListItem>
      <ListItem>Availability: failure of one processor doesn&apos;t end system</ListItem>
      <ListItem>Horizontal scaling</ListItem>
    </List>
    <Text>They have the following disadvantages</Text>
    <List>
      <ListItem>Performance: when programs can be written to run on multiple processors</ListItem>
      <ListItem>Availability: failure of one processor doesn&apos;t end system</ListItem>
      <ListItem>Horizontal scaling</ListItem>
    </List>
    <Text>
      Each processor has it&apos;s own CU, ALU, registers, and generally at least L1 and L2 cache. Processors can
      communicate with each other via shared memory; it can be possible for them to communicate directly. The OS is in
      charge of scheduling and synchronization among processors; the multiple processors are transparent to the user.
    </Text>
    <Text>
      Because processors have their own cache, if they write through to memory, it could invalidate cache in other
      processors. Other processors must be notified their cache is now invalid. This is known as the cache coherence
      problem and is normally solved by hardware rather than the OS.
    </Text>
    <Title order={3}>Multicore computers</Title>
    <Text>
      Multicore computers are also known as a chip multiprocessor. It combines multiple processors (similar to SMPs)
      (known as cores in multicore computers) onto a single bit of silicon (known as die in multicore computers). Each
      core normally contains everything a typical processor would have (ALU, CU, registers, cache, etc).
    </Text>
    <Text>
      Chip multiprocessors became more popular as it became harder to increase the performance of single
      microprocessors.
    </Text>

    <Title order={2}>Performance Characteristics of Two-Level Memories</Title>
    <Text></Text>

    <Title order={2}>Review Questions</Title>
    <Accordion></Accordion>
  </>
);

export default function Notes() {
  return (
    <Container size={"sm"} mt={"xl"} mb={"xl"}>
      <Stack>{pageContent}</Stack>
    </Container>
  );
}
